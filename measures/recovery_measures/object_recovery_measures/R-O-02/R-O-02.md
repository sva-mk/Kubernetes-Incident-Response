# R-O-02 Object Recovery through the Recovery of WorkerNodes

In addition to configuration, the necessary resources should also be restored during the recovery phase to best reconstruct the operational state before the incident [[MPM14, p.623](http://cisweb.bristolcc.edu/~ik/Download/Forensic/Incident_Response_Computer_Forensics_3rd_Edition.pdf)]. Particularly in the case of cryptojacking incidents, it may be financially unfeasible to completely rebuild the infrastructure, meaning that previously removed components must be restored. This ultimately requires previously shut-down worker nodes to be reintegrated into the cluster during the recovery phase. In this case, new worker nodes must be created as part of the preparations, with the procedure varying significantly depending on the infrastructure platform used [[The24q](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/); [Goo24a](https://cloud.google.com/kubernetes-engine/docs/how-to/node-pools)]. When using VMs or a cloud-based Kubernetes environment, a new node can often be created from a predefined template [[Bro20](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vm_admin.doc/GUID-E9EAF7AC-1C08-441A-AB80-0BAA1EAF9F0A.html); [Goo24a](https://cloud.google.com/kubernetes-engine/docs/how-to/node-pools)]. However, when using physical servers, a complete reconfiguration of the operating system is required [[MPM14, p.615](http://cisweb.bristolcc.edu/~ik/Download/Forensic/Incident_Response_Computer_Forensics_3rd_Edition.pdf)]. Regardless of the infrastructure platform, the new node must be configured in the same way as the other worker nodes. This includes installing the same container runtime and the identical Kubernetes engine, with both the distribution used and its version being important. The necessary steps can be found in the respective documentation of the examined Kubernetes distributions [[The24s](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/); [The24q](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/); [SUS24d](https://docs.rke2.io/); [K3s24c](https://docs.k3s.io/installation/requirements)]. After preconfiguring the node, it can be integrated into the cluster using the following procedure, where only the necessary actions for a cluster created with kubeadm are considered:

1. Check the network connection to the Kubernetes API (see Listing 1 line 1) [[The23w](https://kubernetes.io/docs/reference/using-api/health-checks/)].
   - Although the `ping` command-line tool is often used for this, it should also be verified whether access to the Kubernetes API is possible using `curl` or a similar tool.

2. Create a token to add the node to the cluster (see Listing 1 line 2) [[The24q](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)].
   - At this point, it is possible to reuse the original token, but if it cannot be ruled out that it was compromised during the incident, it should be removed using the command in line 3.

3. Add the new node to the cluster (see Listing 1 line 4) [[The24q](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)].
   - The command in line 4 must be executed directly on the reconstructed node.
   - The output of the command from step two can be used as a template.

4. Verify the nodeâ€™s integration into the cluster (see Listing 1 line 5) [[The24q](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)].
   - If the node is not marked as ready, further investigation of the disrupting factors is required.

##### Listing 1: Commands for restoring worker nodes according to [[The24q](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/); [The24s](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)]

```bash
curl -k https://<master-node-ip>:6443/healthz
sudo kubeadm token create --print-join-command
sudo kubeadm token delete <token>
sudo kubeadm join <master-node-ip>:6443 --token <token> --discovery-token-ca-cert-hash <hash>
kubectl get nodes -o wide
```

## Evaluation

The following table, entitled *Evaluation of measure R-O-02*, provides an overview of the evaluation of the aforementioned measure. The composition of the overall rating is then described in detail.

#### Table: Evaluation of R-O-02

| Criteria           | Result |
| ------------------ | ------ |
| Applicability      | 1      |
| Preparation Effort | 2      |
| Complexity         | 3      |
| Coverage           | 1      |
| Business Impact    | 5      |
| Visibility         | 5      |
| Resilience         | 3      |
| Reproducibility    | 5      |
| Interoperability   | 4      |
| Overall Rating     | 3,05   |

The evaluation of the measure was carried out in the experimental environment according to the aforementioned guidelines. In the process, one of the worker nodes was initially removed from the cluster using the procedure from Measure [E-P-03](/measures/eradication_measures/process_eradication_measures/E-P-03/E-P-03.md), and the kubeadm configuration on that node was completely removed using the command from Listing 2. The node was then reintroduced to the cluster, where it was found that the measure had very low applicability (1), as it could only be successfully executed in two of the four test scenarios, and interaction with individual nodes was required. However, the measure showed very high reproducibility (5), as identical results were achieved in all five successful executions. The results, similar to the previous measure, showed high interoperability (4), as the results could be output both as typical command-line outputs and in YAML format, and were highly practical [[The24u](https://kubernetes.io/docs/reference/kubectl/quick-reference/)]. The preparation effort for the measure varies significantly depending on the infrastructure platform used and the Kubernetes distribution deployed. However, since the configuration of the operating system and the installation of the necessary Kubernetes components involve several actions, and the individual commands often need to be tailored to the environment, the overall preparation effort can be considered high (2) [[The24q](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/); [The24s](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)]. At the same time, the complexity of the measure is at a medium level (3), as four actions in total are required to integrate a node into a cluster within the examined procedure. The business impact is rated as very low (5) based on Figure 1, though it must be emphasized that the measure tends to have a positive impact on the operational state by restoring resources. The visibility of this measure is also very low (5), as no changes are made to the compromised system components.

Similar to Measures [E-P-03](/measures/eradication_measures/process_eradication_measures/E-P-03/E-P-03.md) and [C-P-01](/measures/containment_measures/process_isolation_measures/C-P-01/C-P-01.md), the resilience of this measure is rated only as satisfactory (3), as, after a thorough investigation of the compromised cluster, additional applications could potentially have been compromised unnoticed by the attackers, with their pods being placed on the newly integrated node. The coverage of the measure is also very low (1), as in the examined attack scenario, node compromise is explicitly excluded, but in a real-world situation, this cannot generally be ruled out from the perspective of the affected organization. Overall, the measure is given a satisfactory rating (3.05).

##### Listing 2: Instruction to prepare the assessment of R-O-02 according to [The24q]

```bash
sudo kubeadm reset --force
```
##### Figure 1: Business impact of measure R-O-02
![Business impact of measure R-O-02](Dependency-Modeling-Diagram_R-O-02.png)