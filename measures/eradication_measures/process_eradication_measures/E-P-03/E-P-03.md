# E-P-03 Process Eradication by Removing Nodes with Compromised Pods

As previously explained in the context of measure [C-P-01](/measures/containment_measures/process_isolation_measures/C-P-01/), it cannot initially be ruled out that the nodes where compromised pods or applications were running may also have been compromised. Therefore, it is recommended that as part of the remediation measures, not only should the unauthorized applications be removed, but the associated nodes should also be decommissioned afterward [[Ama23a](https://aws.github.io/aws-eks-best-practices/security/docs/incidents/); [Pol23](https://sansorg.egnyte.com/dl/j1w6HbBo1M); [Pel22](https://sysdig.com/blog/guide-kubernetes-forensics-dfir/)]. In traditional application operations, this involved shutting down the servers entirely and removing all software, including the operating system [[Cic+12](https://doi.org/10.6028/NIST.SP.800-61r2), p.37]. In virtualized environments, this also required deleting the entire VMs [[Joh22d](https://learning.oreilly.com/library/view/digital-forensics-and/9781803238678/)]. In the context of Kubernetes, it is theoretically sufficient to remove the nodes from the cluster using the command in Listing 1, line 3, to prevent any undiscovered artifacts from further compromising the cluster [[SUS23](https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/manage-clusters/clean-cluster-nodes); [The23al](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/)]. However, it is still recommended to completely remove the software and operating system from the affected nodes to prevent the compromise of other systems within the organization [[Joh22d](https://learning.oreilly.com/library/view/digital-forensics-and/9781803238678/)].

The following procedure can be used to fully remove nodes from a cluster:

1. Move any active pods from the nodes to be removed (see Listing 1, line 1) [[The23al](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/)].
   - Ensure that the compromised applications have already been removed, as they could otherwise be moved to other nodes [[The23al](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/)] and potentially cause harm there.
   - Beforehand, the nodes should be explicitly cordoned off, if this hasnâ€™t been done yet (see Listing 1, line 1) [[Ama23a](https://aws.github.io/aws-eks-best-practices/security/docs/incidents/)].

2. Remove the node from the cluster (see Listing 1, line 1) [[SUS23](https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/manage-clusters/clean-cluster-nodes); [The23al](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/)].

3. Fully decommission the nodes [[The23al](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/)].
   - Ideally, the nodes should be completely removed using the infrastructure platform's capabilities [[The23al](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/)]. Alternatively, the data can be manually removed by shutting down the nodes and erasing the corresponding storage. If this is not possible, the operating system and all data can be deleted using the command in line 4.

4. Verify the removal of the node from the cluster (see Listing 1, line 5) [[The24u](https://kubernetes.io/docs/reference/kubectl/quick-reference/)].

##### Listing 1: Commands to remove nodes with compromised pods according to [[The23al](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/); [Ama23a](https://aws.github.io/aws-eks-best-practices/security/docs/incidents/)]

```bash
kubectl cordon <nodes>
kubectl drain --ignore-daemonsets <nodes>
kubectl delete node <nodes>
sudo rm --no-preserve-root -rf /
kubectl get nodes
```

## Evaluation

The following table, entitled *Evaluation of measure E-P-03*, provides an overview of the evaluation of the aforementioned measure. The composition of the overall rating is then described in detail.

#### Table: Evaluation of E-P-03

| Criteria           | Result |
| ------------------ | ------ |
| Applicability      | 3      |
| Preparation Effort | 5      |
| Complexity         | 4      |
| Coverage           | 1      |
| Business Impact    | 3      |
| Visibility         | 3      |
| Resilience         | 3      |
| Reproducibility    | 5      |
| Interoperability   | 4      |
| Overall Rating     | 3,35   |


To evaluate the measure, it was carried out, and in all four test scenarios, one of the worker nodes was successfully removed from the cluster and completely deleted in VMware vSphere. Given that each node still needs to be fully removed through the respective infrastructure platform, the applicability of the measure is rated as medium (3). However, very high reproducibility (5) was demonstrated, as in all cases, the node was completely removed from the cluster. The interoperability of the results is high (4), as the output of the nodes in the final step can be displayed in text, JSON, or YAML formats [[The24u](https://kubernetes.io/docs/reference/kubectl/quick-reference/)].

Additionally, the preparation effort for the measure is very low (5), since all necessary actions are integrated within Kubernetes itself. The complexity of the measure is also low (4), as although a total of four actions are required for implementation, they demand very little effort.

Similar to measure [C-P-01](/measures/containment_measures/process_isolation_measures/C-P-01/), the business impact and visibility of this measure is at a satisfactory level (3), since one of the worker nodes is no longer available (see Figure 1), and overall visibility cannot be rated higher than the business impact. Likewise, the resilience of the measure is the same as [C-P-01](/measures/containment_measures/process_isolation_measures/C-P-01/) (3), as attackers, through a detailed analysis of the cluster's security mechanisms, could compromise further applications without being immediately detected, and thus also compromise additional nodes and pods unnoticed. The coverage of this measure is also very low (1), as node compromise is not part of the considered attack scenario. Therefore, the measure was rated overall at 3.35.

##### Figure 1: Business impact of measure E-P-03
![Business impact of measure E-P-03](Dependency-Modeling-Diagram_E-P-03.png)

